{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22499e0-d789-4a08-9876-a1e5ed73db30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 4, saw 292\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone loading fact_uk_air_quality_daily\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     pm25_raw = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPM25_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m     no2_raw  = pd.read_csv(NO2_PATH, low_memory=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    160\u001b[39m     pm25 = standardise(pm25_raw, \u001b[33m\"\u001b[39m\u001b[33mpm25\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Air_Pollution\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Air_Pollution\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Air_Pollution\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Air_Pollution\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._first_chunk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:820\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:914\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 1 fields in line 4, saw 292\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from datetime import date\n",
    "\n",
    "PM25_PATH = \"C:/Users/Krist/Documents/Work/Data Science/projects/Air_Quality/data/raw/pollution/PM2_5_2022.csv\"\n",
    "NO2_PATH  = \"C:/Users/Krist/Documents/Work/Data Science/projects/Air_Quality/data/raw/pollution/Nitrogen_Dioxide_2022.csv\"\n",
    "\n",
    "BASE_URL = \"https://api.openaq.org/v3\"\n",
    "PG_DSN=\"dbname=airquality user=postgres password=Milian112! host=localhost port=5432\"\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2022-09-01\")\n",
    "\n",
    "def pg_conn():\n",
    "    return psycopg2.connect(PG_DSN)\n",
    "\n",
    "def guess_col(df, candidates):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols:\n",
    "            return cols[cand.lower()]\n",
    "    # fallback: substring match\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if any(k.lower() in cl for k in candidates):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def standardise(df: pd.DataFrame, pollutant: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attempt to standardise to:\n",
    "      site_id, site_name, latitude, longitude, date_utc, value, units\n",
    "    Works even if your CSV uses different column names.\n",
    "    \"\"\"\n",
    "    # Common column possibilities\n",
    "    c_site_id   = guess_col(df, [\"site_id\", \"siteid\", \"station_id\", \"stationid\", \"location_id\", \"locationid\", \"Site\"])\n",
    "    c_site_name = guess_col(df, [\"site_name\", \"sitename\", \"station_name\", \"location\", \"name\"])\n",
    "    c_lat       = guess_col(df, [\"latitude\", \"lat\"])\n",
    "    c_lon       = guess_col(df, [\"longitude\", \"lon\", \"lng\", \"long\"])\n",
    "    c_date      = guess_col(df, [\"date_utc\", \"date\", \"day\", \"datetime\", \"period\"])\n",
    "    c_value     = guess_col(df, [\"value\", \"mean\", \"avg\", \"concentration\"])\n",
    "    c_units     = guess_col(df, [\"units\", \"unit\"])\n",
    "\n",
    "    if not c_date or not c_value:\n",
    "        raise RuntimeError(\n",
    "            f\"Could not detect required columns in {pollutant} CSV. \"\n",
    "            f\"Need at least a date and value column. Found columns: {list(df.columns)[:40]}\"\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"pollutant\"] = pollutant\n",
    "    out[\"site_id\"] = df[c_site_id].astype(str) if c_site_id else None\n",
    "    out[\"site_name\"] = df[c_site_name].astype(str) if c_site_name else None\n",
    "    out[\"latitude\"] = pd.to_numeric(df[c_lat], errors=\"coerce\") if c_lat else None\n",
    "    out[\"longitude\"] = pd.to_numeric(df[c_lon], errors=\"coerce\") if c_lon else None\n",
    "\n",
    "    # Parse date\n",
    "    d = pd.to_datetime(df[c_date], errors=\"coerce\", utc=True)\n",
    "    out[\"date_utc\"] = d.dt.date\n",
    "\n",
    "    out[\"value\"] = pd.to_numeric(df[c_value], errors=\"coerce\")\n",
    "    out[\"units\"] = df[c_units].astype(str) if c_units else None\n",
    "\n",
    "    # Keep a raw copy (optional but handy)\n",
    "    out[\"raw\"] = df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Filter date\n",
    "    out = out[out[\"date_utc\"].notna()]\n",
    "    out = out[pd.to_datetime(out[\"date_utc\"]) >= START_DATE]\n",
    "\n",
    "    # If site_id is missing, create a deterministic ID from lat/lon + name\n",
    "    if out[\"site_id\"].isna().all():\n",
    "        # fallback id\n",
    "        out[\"site_id\"] = (\n",
    "            out[\"site_name\"].fillna(\"unknown\").astype(str)\n",
    "            + \"_\"\n",
    "            + out[\"latitude\"].fillna(0).round(5).astype(str)\n",
    "            + \"_\"\n",
    "            + out[\"longitude\"].fillna(0).round(5).astype(str)\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "def truncate_tables(conn):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"TRUNCATE TABLE stg_uk_air_quality_daily;\")\n",
    "        cur.execute(\"TRUNCATE TABLE fact_uk_air_quality_daily;\")\n",
    "    conn.commit()\n",
    "\n",
    "def insert_stage(conn, df: pd.DataFrame):\n",
    "    rows = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        rows.append((\n",
    "            r.pollutant,\n",
    "            r.site_id,\n",
    "            r.site_name,\n",
    "            r.latitude,\n",
    "            r.longitude,\n",
    "            r.date_utc,\n",
    "            r.value,\n",
    "            r.units,\n",
    "            json.dumps(r.raw),\n",
    "        ))\n",
    "\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO stg_uk_air_quality_daily\n",
    "      (pollutant, site_id, site_name, latitude, longitude, date_utc, value, units, raw)\n",
    "    VALUES %s;\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        psycopg2.extras.execute_values(cur, sql, rows, page_size=5000)\n",
    "    conn.commit()\n",
    "\n",
    "def build_fact(conn):\n",
    "    \"\"\"\n",
    "    Build deduped fact table:\n",
    "    - For duplicates within (pollutant, site_id, date_utc), keep max(value) as a simple deterministic rule.\n",
    "      (We can change this to avg/last if you prefer.)\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO fact_uk_air_quality_daily\n",
    "              (pollutant, site_id, date_utc, value, units, latitude, longitude, site_name, geom)\n",
    "            SELECT\n",
    "              pollutant,\n",
    "              site_id,\n",
    "              date_utc,\n",
    "              MAX(value) AS value,\n",
    "              MAX(units) AS units,\n",
    "              MAX(latitude) AS latitude,\n",
    "              MAX(longitude) AS longitude,\n",
    "              MAX(site_name) AS site_name,\n",
    "              CASE\n",
    "                WHEN MAX(latitude) IS NOT NULL AND MAX(longitude) IS NOT NULL\n",
    "                THEN ST_SetSRID(ST_MakePoint(MAX(longitude), MAX(latitude)), 4326)\n",
    "                ELSE NULL\n",
    "              END AS geom\n",
    "            FROM stg_uk_air_quality_daily\n",
    "            WHERE site_id IS NOT NULL\n",
    "              AND date_utc IS NOT NULL\n",
    "            GROUP BY pollutant, site_id, date_utc\n",
    "            ON CONFLICT (pollutant, site_id, date_utc) DO UPDATE SET\n",
    "              value=EXCLUDED.value,\n",
    "              units=EXCLUDED.units,\n",
    "              latitude=EXCLUDED.latitude,\n",
    "              longitude=EXCLUDED.longitude,\n",
    "              site_name=EXCLUDED.site_name,\n",
    "              geom=EXCLUDED.geom,\n",
    "              loaded_at=now();\n",
    "        \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "def main():\n",
    "    pm25_raw = pd.read_csv(PM25_PATH, low_memory=False)\n",
    "    no2_raw  = pd.read_csv(NO2_PATH, low_memory=False)\n",
    "\n",
    "    pm25 = standardise(pm25_raw, \"pm25\")\n",
    "    no2  = standardise(no2_raw, \"no2\")\n",
    "\n",
    "    print(\"PM2.5 rows after filtering:\", len(pm25))\n",
    "    print(\"NO2 rows after filtering:\", len(no2))\n",
    "\n",
    "    with pg_conn() as conn:\n",
    "        # If you truly want a clean reload each time:\n",
    "        truncate_tables(conn)\n",
    "\n",
    "        insert_stage(conn, pm25)\n",
    "        insert_stage(conn, no2)\n",
    "\n",
    "        build_fact(conn)\n",
    "\n",
    "    print(\"Done loading fact_uk_air_quality_daily\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
