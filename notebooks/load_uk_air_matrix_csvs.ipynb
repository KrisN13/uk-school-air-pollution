{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973f3d5f-4713-422d-a203-eceb626d78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing PM2.5 file...\n",
      "PM2.5 long rows: 1497415\n",
      "Parsing NO2 file...\n",
      "NO2 long rows: 3544875\n",
      "Done. Loaded dim_uk_air_sites, fact_uk_air_hourly, fact_uk_air_daily\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "PM25_PATH = \"C:/Users/Krist/Documents/Work/Data Science/projects/Air_Quality/data/raw/pollution/PM2_5_2022.csv\"\n",
    "NO2_PATH  = \"C:/Users/Krist/Documents/Work/Data Science/projects/Air_Quality/data/raw/pollution/Nitrogen_Dioxide_2022.csv\"\n",
    "\n",
    "PG_DSN=\"dbname=airquality user=postgres password=Milian112! host=localhost port=5432\"\n",
    "if not PG_DSN:\n",
    "    raise RuntimeError(\"Set PG_DSN in environment (PG_DSN) or hardcode it in the script.\")\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2022-09-01\", tz=\"UTC\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def pg_conn():\n",
    "    return psycopg2.connect(PG_DSN)\n",
    "\n",
    "def norm(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    return s if s != \"\" else None\n",
    "\n",
    "def parse_status_units_method(status_text: str):\n",
    "    \"\"\"\n",
    "    Status column often looks like:\n",
    "      'V ugm-3 (BAM)'\n",
    "      'V ugm-3 (Ref.eq)'\n",
    "      'No data,V ugm-3 (BAM)' doesn't happen here; 'No data' is in value col.\n",
    "    We extract:\n",
    "      status_code = first token\n",
    "      units = something like 'ugm-3'\n",
    "      method = content in parentheses\n",
    "    \"\"\"\n",
    "    if not status_text:\n",
    "        return None, None, None\n",
    "    t = status_text.strip()\n",
    "    # status code = first token (V/P/N/S etc) if present\n",
    "    m = re.match(r\"^([A-Za-z]+)\\s+(.*)$\", t)\n",
    "    status_code = None\n",
    "    rest = t\n",
    "    if m:\n",
    "        status_code = m.group(1)\n",
    "        rest = m.group(2)\n",
    "\n",
    "    units = None\n",
    "    method = None\n",
    "    # method in parentheses\n",
    "    m2 = re.search(r\"\\(([^)]+)\\)\", rest)\n",
    "    if m2:\n",
    "        method = m2.group(1).strip()\n",
    "        rest2 = re.sub(r\"\\([^)]+\\)\", \"\", rest).strip()\n",
    "    else:\n",
    "        rest2 = rest.strip()\n",
    "\n",
    "    # units is whatever remains (e.g. ugm-3)\n",
    "    if rest2:\n",
    "        units = rest2\n",
    "\n",
    "    return status_code, units, method\n",
    "\n",
    "def read_metadata_rows(path: str):\n",
    "    \"\"\"\n",
    "    Reads the first 10 lines of the file and returns a dict of metadata lists aligned by site index.\n",
    "    Expected structure based on your file:\n",
    "      line1: title\n",
    "      line2: notes\n",
    "      line3: status legend\n",
    "      line4: Site Name,,\"A\",,\"B\",,...\n",
    "      line5: Latitude,,...,,\n",
    "      line6: Longitude,,...,,\n",
    "      line7: Site Type,,...,,\n",
    "      line8: Zone,,...,,\n",
    "      line9: Agglomeration,,...,,\n",
    "      line10: Local Authority,,...,,\n",
    "      line11: Date,Time,<value>,Status,<value>,Status,...\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        lines = [next(reader) for _ in range(10)]\n",
    "\n",
    "    def extract_every_other(row):\n",
    "        # After the first cell label, the pattern is: blank, value, blank, value...\n",
    "        # In your file it looks like: label,,\"Site1\",,\"Site2\",,...\n",
    "        # csv.reader turns ,, into empty strings.\n",
    "        # The site entries are at indices 2,4,6,...\n",
    "        out = []\n",
    "        for i in range(2, len(row), 2):\n",
    "            out.append(norm(row[i]))\n",
    "        return out\n",
    "\n",
    "    meta = {\n",
    "        \"site_name\": extract_every_other(lines[3]),\n",
    "        \"latitude\": extract_every_other(lines[4]),\n",
    "        \"longitude\": extract_every_other(lines[5]),\n",
    "        \"site_type\": extract_every_other(lines[6]),\n",
    "        \"zone\": extract_every_other(lines[7]),\n",
    "        \"agglomeration\": extract_every_other(lines[8]),\n",
    "        \"local_authority\": extract_every_other(lines[9]),\n",
    "    }\n",
    "\n",
    "    # Convert lat/lon to float where possible\n",
    "    def to_float_list(xs):\n",
    "        out = []\n",
    "        for x in xs:\n",
    "            try:\n",
    "                out.append(float(x) if x is not None else None)\n",
    "            except ValueError:\n",
    "                out.append(None)\n",
    "        return out\n",
    "\n",
    "    meta[\"latitude\"] = to_float_list(meta[\"latitude\"])\n",
    "    meta[\"longitude\"] = to_float_list(meta[\"longitude\"])\n",
    "\n",
    "    n_sites = len(meta[\"site_name\"])\n",
    "    return meta, n_sites\n",
    "\n",
    "def build_site_keys(n_sites: int):\n",
    "    # Stable keys by index (site_0001, site_0002...)\n",
    "    return [f\"site_{i:04d}\" for i in range(1, n_sites + 1)]\n",
    "\n",
    "def load_matrix_file_to_long(path: str, pollutant: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the matrix CSV as:\n",
    "      Date, Time, value1, status1, value2, status2, ...\n",
    "    Using header=None and skipping first 10 lines.\n",
    "    Drops the header row at the top of the data block (line 11 in file).\n",
    "    \"\"\"\n",
    "    meta, n_sites = read_metadata_rows(path)\n",
    "    site_keys = build_site_keys(n_sites)\n",
    "\n",
    "    # Create deterministic column names: date,time then per site: v/status\n",
    "    cols = [\"date\", \"time\"]\n",
    "    for sk in site_keys:\n",
    "        cols += [f\"{sk}__value\", f\"{sk}__status\"]\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        skiprows=10,       # skip metadata lines\n",
    "        header=None,       # file has a header row, but it's messy/repeated\n",
    "        names=cols,\n",
    "        engine=\"python\",\n",
    "    )\n",
    "\n",
    "    # First row here is actually the \"Date,Time,PM2.5...,Status,...\" header line -> drop it\n",
    "    df = df[df[\"date\"] != \"Date\"].copy()\n",
    "\n",
    "    # Parse datetime (your file uses ISO date + HH:MM:SS)\n",
    "    dt_utc = pd.to_datetime(\n",
    "    df[\"date\"] + \" \" + df[\"time\"],\n",
    "    errors=\"coerce\",\n",
    "    utc=True,\n",
    "    dayfirst=True\n",
    "    )\n",
    "\n",
    "    now_utc = pd.Timestamp.utcnow()\n",
    "    df[\"datetime_utc\"] = dt_utc\n",
    "    df = df[df[\"datetime_utc\"].notna()]\n",
    "    df = df[df[\"datetime_utc\"] <= now_utc + pd.Timedelta(days=1)]\n",
    "    df = df[df[\"datetime_utc\"] >= START_DATE]\n",
    "\n",
    "    # Melt into long (site_key, value, status_text)\n",
    "    out_rows = []\n",
    "    for sk in site_keys:\n",
    "        vcol = f\"{sk}__value\"\n",
    "        scol = f\"{sk}__status\"\n",
    "\n",
    "        tmp = df[[\"datetime_utc\", vcol, scol]].copy()\n",
    "        tmp.rename(columns={vcol: \"value_raw\", scol: \"status_text\"}, inplace=True)\n",
    "        tmp[\"site_key\"] = sk\n",
    "        tmp[\"pollutant\"] = pollutant\n",
    "\n",
    "        # value: numeric where possible; treat \"No data\" as null\n",
    "        tmp[\"value\"] = pd.to_numeric(tmp[\"value_raw\"].replace({\"No data\": None}), errors=\"coerce\")\n",
    "\n",
    "        out_rows.append(tmp[[\"pollutant\", \"site_key\", \"datetime_utc\", \"value\", \"status_text\"]])\n",
    "\n",
    "    long_df = pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "    # Extract units/method from status_text\n",
    "    parsed = long_df[\"status_text\"].fillna(\"\").apply(parse_status_units_method)\n",
    "    long_df[\"status_code\"] = [p[0] for p in parsed]\n",
    "    long_df[\"units\"] = [p[1] for p in parsed]\n",
    "    long_df[\"method\"] = [p[2] for p in parsed]\n",
    "\n",
    "    # raw json for provenance (optional)\n",
    "    long_df[\"raw\"] = None\n",
    "\n",
    "    return long_df, meta, site_keys\n",
    "\n",
    "def upsert_sites(conn, meta, site_keys):\n",
    "    rows = []\n",
    "    for i, sk in enumerate(site_keys):\n",
    "        rows.append((\n",
    "            sk,\n",
    "            meta[\"site_name\"][i],\n",
    "            meta[\"latitude\"][i],\n",
    "            meta[\"longitude\"][i],\n",
    "            meta[\"site_type\"][i],\n",
    "            meta[\"zone\"][i],\n",
    "            meta[\"agglomeration\"][i],\n",
    "            meta[\"local_authority\"][i],\n",
    "        ))\n",
    "\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO dim_uk_air_sites\n",
    "      (site_key, site_name, latitude, longitude, site_type, zone, agglomeration, local_authority, geom)\n",
    "    VALUES %s\n",
    "    ON CONFLICT (site_key) DO UPDATE SET\n",
    "      site_name=EXCLUDED.site_name,\n",
    "      latitude=EXCLUDED.latitude,\n",
    "      longitude=EXCLUDED.longitude,\n",
    "      site_type=EXCLUDED.site_type,\n",
    "      zone=EXCLUDED.zone,\n",
    "      agglomeration=EXCLUDED.agglomeration,\n",
    "      local_authority=EXCLUDED.local_authority,\n",
    "      geom=EXCLUDED.geom,\n",
    "      loaded_at=now();\n",
    "    \"\"\"\n",
    "\n",
    "    # Build geom in SQL so we don't need shapely\n",
    "    templ = \"(%s,%s,%s,%s,%s,%s,%s,%s, CASE WHEN %s IS NOT NULL AND %s IS NOT NULL THEN ST_SetSRID(ST_MakePoint(%s,%s),4326) ELSE NULL END)\"\n",
    "\n",
    "    # Expand each row to include lon/lat for geom\n",
    "    expanded = []\n",
    "    for (sk, name, lat, lon, st, zone, agg, la) in rows:\n",
    "        expanded.append((sk, name, lat, lon, st, zone, agg, la, lat, lon, lon, lat))\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        psycopg2.extras.execute_values(cur, sql, expanded, template=templ, page_size=1000)\n",
    "    conn.commit()\n",
    "\n",
    "def upsert_hourly(conn, df_long: pd.DataFrame):\n",
    "    rows = []\n",
    "    for r in df_long.itertuples(index=False):\n",
    "        rows.append((\n",
    "            r.pollutant,\n",
    "            r.site_key,\n",
    "            r.datetime_utc.to_pydatetime(),\n",
    "            None if pd.isna(r.value) else float(r.value),\n",
    "            norm(r.status_text),\n",
    "            norm(r.units),\n",
    "            norm(r.method),\n",
    "            json.dumps({\"status_code\": r.status_code})  # keep small; expand later if needed\n",
    "        ))\n",
    "\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO fact_uk_air_hourly\n",
    "      (pollutant, site_key, datetime_utc, value, status_text, units, method, raw)\n",
    "    VALUES %s\n",
    "    ON CONFLICT (pollutant, site_key, datetime_utc) DO UPDATE SET\n",
    "      value=EXCLUDED.value,\n",
    "      status_text=EXCLUDED.status_text,\n",
    "      units=EXCLUDED.units,\n",
    "      method=EXCLUDED.method,\n",
    "      raw=EXCLUDED.raw,\n",
    "      loaded_at=now();\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        psycopg2.extras.execute_values(cur, sql, rows, page_size=5000)\n",
    "    conn.commit()\n",
    "\n",
    "def rebuild_daily(conn):\n",
    "    # Full rebuild for simplicity\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"TRUNCATE TABLE fact_uk_air_daily;\")\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO fact_uk_air_daily\n",
    "              (pollutant, site_key, date_utc, avg_value, n_hours, pct_valid)\n",
    "            SELECT\n",
    "              pollutant,\n",
    "              site_key,\n",
    "              (datetime_utc AT TIME ZONE 'UTC')::date AS date_utc,\n",
    "              AVG(value) AS avg_value,\n",
    "              COUNT(*) AS n_hours,\n",
    "              100.0 * AVG(CASE WHEN value IS NOT NULL THEN 1 ELSE 0 END) AS pct_valid\n",
    "            FROM fact_uk_air_hourly\n",
    "            GROUP BY pollutant, site_key, (datetime_utc AT TIME ZONE 'UTC')::date;\n",
    "        \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "def main():\n",
    "    print(\"Parsing PM2.5 file...\")\n",
    "    pm25_long, pm25_meta, pm25_keys = load_matrix_file_to_long(PM25_PATH, \"pm25\")\n",
    "    print(\"PM2.5 long rows:\", len(pm25_long))\n",
    "\n",
    "    print(\"Parsing NO2 file...\")\n",
    "    no2_long, no2_meta, no2_keys = load_matrix_file_to_long(NO2_PATH, \"no2\")\n",
    "    print(\"NO2 long rows:\", len(no2_long))\n",
    "\n",
    "    # Site keys are index-based; both files should have same site ordering, but we upsert from each anyway\n",
    "    with pg_conn() as conn:\n",
    "        upsert_sites(conn, pm25_meta, pm25_keys)\n",
    "        upsert_sites(conn, no2_meta, no2_keys)\n",
    "\n",
    "        upsert_hourly(conn, pm25_long)\n",
    "        upsert_hourly(conn, no2_long)\n",
    "\n",
    "        rebuild_daily(conn)\n",
    "\n",
    "    print(\"Done. Loaded dim_uk_air_sites, fact_uk_air_hourly, fact_uk_air_daily\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
